%%%% ijcai26.tex

\typeout{IJCAI--ECAI 26 Instructions for Authors}

% These are the instructions for authors for IJCAI--ECAI 26.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai26.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai26}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsfonts}

% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\pdfinfo{
/TemplateVersion (IJCAI.2026.0)
}

\title{Hi-Drum: Drum Transcription Method Based on
Large-Scale Pre-trained Models}

% Single author syntax
\author{
    Anonymous IJCAI submission
}

\begin{document}

\maketitle

\begin{abstract}
While Automatic Drum Transcription (ADT) is a prerequisite for robust music rhythmic analysis in the field of automatic music transcription, the task remains unsolved to date. The difficulty stems from the inherent characteristics of percussive sound sources: unlike melodic instruments such as violins and guitars, drum sounds exhibit transient, broadband energy distributions and lack well-defined harmonic series, which hinders the extraction of symbolic representations from music audio. Moreover, existing ADT methods often overlook the structural nuances of musical performance, failing to capture pattern consistency within musical sections and the evolution of fill passages between them.
To address these limitations, we propose Hi-Drum, a novel paradigm combining a self-supervised music understanding model and a Large Language Model (LLM), integrated with a Hierarchical Drum Module (HDM). The HDM adopts a three-branch parallel architecture to explicitly model coarse-to-fine features, enabling effective capture of both local variations and global similarities. To further explore the critical roles of dynamics and timing, we design a multi-task fine-tuning strategy with three core components of symbolic representations: onset, velocity and frame detection as auxiliary tasks.
Extensive experiments conducted on multiple open-source datasets demonstrate that Hi-Drum achieves state-of-the-art performance (Note F1 91.31).
\end{abstract}

\section{Introduction}
Automatic Music Transcription (AMT) is a core task in Music Information Retrieval (MIR) \cite{benetos2018automatic}. Its mainstream pipeline extracts spectrograms from audio files, feeds them into transcription models, and outputs standardized symbolic music representations (e.g., MIDI, sheet music). As a key bridge between symbolic music and music generation domains, AMT provides technical support for music creation, copyright management, and plays an irreplaceable role in music education.

\begin{figure}[t]
\centerline{\includegraphics[width=\columnwidth]{picture/picture1 (1).png}}
\caption{The flowchart of the general Automatic Music Transcription (AMT) process.}
\label{fig:cqt_comparison}
\end{figure}

As a subtask of AMT, Automatic Drum Transcription (ADT) is essentially the process of converting an audio signal into a symbolic representation of the drum events, typically in the form of a MIDI file \cite{Southall2016ADT}\cite{adt_review}. As shown in Figure 1, transcription modeling for other instruments relies on the relationship between Fundamental Frequency and Harmonics, enabling the identification of onset, offset, and pitch by extracting harmonic structures \cite{hawthorne2018onsets,}\cite{hfsformer}. However, as unpitched percussion instruments, drums exhibit noise-like characteristics and lack clear harmonic structures, making it difficult to perform transcription modeling by extracting harmonic structures. This presents a perplexing issue in ADT. 

% Transcription of other instruments requires extracting onset, offset, pitch, and velocity to characterize a note; in contrast, ADT does not need offset detection. This difference stems from distinct Attack-Decay-Sustain (ADS) envelopes: the sustain phase of other instruments is performer-controlled, while drum sounds decay rapidly after attack.
\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{picture/pipline.png}
\caption{Overall model architecture. The audio signal is first extracted for general music representations by the MERT encoder. Then it undergoes multi-scale feature aggregation through the three-branch HDM module. Next, sequence modeling is performed by the Qwen decoder. Finally, the final drum score prediction is generated through the multi-task fine-tuning module.}  
\label{fig1}
\end{figure*}
In recent years, ADT has seen some progress. Vogl et al. proposed a Joint Beat and Drum Modeling approach \cite{joint_beat_drum}, which improved onset detection accuracy by leveraging the rhythmic dependency between beats and drum hits. Wei et al. introduced a large-scale data-driven method \cite{audio2midi_adt} to handle polyphonic drum transcription using massive audio-to-MIDI aligned datasets. Advancements in foundation models have also inspired new architectural designs. Li et al. introduced MERT \cite{mert}, a self-supervised encoder capable of extracting rich musical semantics. Du et al. proposed a framework \cite{du2024joint} that combines a pre-trained audio encoder with a large language model decoder, validating the effectiveness of this hybrid architecture in handling complex music information retrieval tasks. Yeung et al. proposed Noise-to-Notes (N2N) \cite{n2n}, redefining ADT as a conditional generation task using diffusion models and achieving state-of-the-art (SOTA) performance. However, the above studies ignore the local variations and global similarities in drum performances. For instance, drummers often introduce unique drum fills and solos to signal transitions (e.g., from Verse to Chorus), while maintaining highly similar drum patterns and grooves within the same section. Current approaches fail to explicitly model the consistency of drum patterns within sections and their evolution between sections. 

To address this problem, we propose Hi-Drum, a novel paradigm for drum transcription based on Large Language Models (LLMs), incorporating an innovative Hierarchical Drum Module (HDM). The HDM employs a three-branch parallel architecture---Parallel Convolution, Multi-scale Dilated Convolution, and Attention---to capture coarse-to-fine features ranging from frame-level and bar-level to global-level, explicitly modeling the local variations and global similarities in drum performances. Based on this, Hi-Drum constructs a comprehensive architecture: Acoustic Music Representation Encoding - Hierarchical Drum Module - Sequence Symbolic Decoding - Multi-task Fine-tuning.
 
 The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item We propose a novel LLM-based drum transcription paradigm that jointly leverages large-scale pre-trained music (MERT) and language (Qwen) models, filling the gap in applying pre-trained large models to drum transcription.
    \item We design the HDM to incorporate prior knowledge of local variations and global similarities in drum performances into drum transcription modeling by capturing coarse-to-fine features.
    \item Extensive evaluations on public datasets (Groove, E-GMD) validate the effectiveness of our framework, achieving new state-of-the-art performance.
\end{itemize}




\section{Method Design}

\subsection{Overall Architecture}
The overall architecture of our proposed Hi-Drum framework is illustrated in Fig. \ref{fig1}. The audio is input to the \textbf{Self-supervised Music Representation Encoder}. This encoder extracts general music representations. These continuous representations are then fed into our proposed \textbf{Hierarchical Drum Module (HDM)}. The HDM employs a three-branch parallel architecture---Parallel Convolution, Multi-scale Dilated Convolution, and Attention---to hierarchically extract frame-level, bar-level, and global-level drum performance features. Subsequently, these extracted hierarchical drum features serve as the input for the \textbf{Large Language Sequence Decoder} for piece-level long-context sequence modeling. Finally, in the \textbf{Multi-task Fine-tuning} module, we utilize the frame branch to assist the onset branch's prediction, and utilize both the frame and onset branches to assist the velocity prediction, resulting in the final structured drum notation output. 




\subsection{Acoustic Music Representation Encoder: MERT}

For the acoustic music representation encoder, this paper adopts MERT\cite{mert}, an industry-leading music understanding foundation model \cite{li2024survey, mertech}. Leveraging ~1.6M hours of unlabeled pre-training data and strong generalization, it achieves SOTA or highly competitive performance across 14 MIR downstream tasks, including music genre classification, beat tracking, key detection, and automatic music tagging.

We input 24kHz raw audio into MERT-v1-330M, a model comprising a CNN feature extractor and a 24-layer Transformer context network with 330M total parameters. The audio is first converted into 75Hz acoustic features via the CNN, then fed into the Transformer for deep contextual modeling, ultimately generating 1024-dimensional, 75Hz music representation information. In this project, all parameters of MERT-v1-330M are frozen, and a learnable weight fusion strategy is adopted to integrate outputs from all 24 Transformer layers---fusing low-level acoustic-physical features (e.g., spectral texture) and high-level abstract musical semantics. This not only adapts to the non-harmonic transient characteristics of drums but also accurately captures rhythmic pattern information, forming a comprehensive multi-scale music representation that is passed to the HDM module.


\subsection{Hierarchical Drum Module}
This paper designs a Hierarchical Drum Module (HDM). This module consists of a Parallel Convolution branch, a Multi-scale Dilated Convolution branch, and an Attention branch.  Each branch is designed based on specific musicological prior knowledge. They model features of different time scales respectively. The aim is to further extract hierarchical music representation information from the general music representations provided by the encoder.

The input feature to HDM is denoted as $H_e \in \mathbb{R}^{T \times F}$, where $T$ denotes the number of time steps and $F$ indicates the feature dimension (1024 in this paper). This tensor is sent in parallel to the following three branches:


\paragraph{Parallel Convolution Branch.}
This branch aims to solve the micro-temporal variation problem of drum onsets. Its design is inspired by the $\pm50$ms time tolerance threshold in mainstream evaluations (such as MIREX). We use two 1D convolution layers in parallel (kernel sizes 3 and 5 respectively). Their receptive fields cover local time windows of about 40ms and 67ms respectively. By fusing these two fields of view of different scales, this branch enhances the model's ability to perceive feature information within adjacent frames. Thus, it improves the precision and robustness of drum onset detection, capturing micro-timing details critical for local accuracy.

\paragraph{Multi-scale Dilated Convolution Branch.}
To extract bar-level features, we design this branch to cover the full bar context. By skipping intermediate frames, dilated convolutions directly correlate intra-bar and cross-bar drum hits, aligning with the sparse and periodic characteristics of drum performance. As illustrated in Figure \ref{fig:bar}, in a 4/4 bar, specific dilation rates connect the downbeat of one bar to the next to model inter-bar dependency, or link strong beats within a bar to capture intra-bar dependency. This enables the encoding of rich intra-bar and cross-bar information by capturing rhythmic features at varying granularities.

\begin{figure}[t]
\centerline{\includegraphics[width=\columnwidth]{picture/bar.png}}
\caption{Schematic diagram of combining music theory knowledge and convolution structure. Dilated kernels (blue blocks) can skip silence to connect distant rhythmic events (e.g., beats within or across bars).}
\label{fig:bar}
\end{figure}

To ensure robustness across varying tempi, we systematically design the dilation rates. A consistent rhythmic pattern (e.g., a 4/4 groove) spans different physical durations depending on the BPM. We calculate the target receptive field $R_{target}$ required to cover a full 4/4 bar as:
\begin{equation}
    R_{target}(\text{BPM}) = \frac{240}{\text{BPM}} \times \frac{f_s}{L_{hop}}
\end{equation}
Given $f_s=24000$ and hop size 320, covering a bar at 60-180 BPM requires receptive fields of roughly 100 to 300 frames. Using a fixed kernel size $k=51$, we derive the required dilation rate $d$:
\begin{equation}
     d \approx \frac{R_{target} - 1}{k - 1} \quad \Rightarrow \quad d \in [2, 6]
\end{equation}
Consequently, we adopt a multi-scale design with $D = \{2, 3, 4, 5, 6\}$. This parallel bank of dilated convolutions serves two purposes: (1) It ensures that at least one branch covers a full bar context for any tempo in the target range; (2) It captures rhythmic features at varying granularities, ranging from local beat subdivisions to rhythmic patterns spanning multiple bars, thereby encoding rich intra-bar and cross-bar information.


\paragraph{Attention Branch.}
To capture global-level musical context and long-range dependencies, we introduce the Attention branch. While convolution operations are inherently limited by their receptive fields, the self-attention mechanism \cite{attention_is_all_you_need} enables the model to perceive the entire sequence simultaneously. Specifically, we employ a lightweight 2-layer Transformer encoder that processes the input tensor $H_e \in \mathbb{R}^{T \times F}$ to extract global semantic features. This global view allows the model to identify repetitive rhythmic patterns and structural consistencies across the entire piece, ensuring that local drum predictions are coherent with the overall musical style and arrangement.

%\paragraph{Learnable Fusion Mechanism.}
%The outputs of the three branches are denoted as $O_{pconv}, O_{dconv}, O_{attn} \in \mathbb{R}^{T \times F}$. These features are integrated through a learnable fusion mechanism. We introduce a set of learnable weights $\mathbf{w} = [w_1, w_2, w_3]$ and use the Softmax function for normalization. The final output $H_a \in \mathbb{R}^{T \times F}$ of HDM is the weighted sum of the outputs of each branch:
%\begin{equation}
%H_a = \sum_{i \in \{pconv, dconv, attn\}} \frac{e^{w_i}}{\sum_{j} e^{w_j}} \cdot O_i
%\end{equation}
%This strategy allows the model to dynamically learn the importance of each branch during training. It adaptively adjusts their contributions, thereby forming a final, information-rich representation for the decoder.






\subsection{Symbolic Decoder: Qwen3}
This paper adopts Qwen3 \cite{qwen3}, a milestone open-source LLM that delivers exceptional performance comparable to top industry models. Its multimodal branch, Qwen-Audio, exhibits powerful music understanding capabilities in MIR tasks such as instrument identification and note analysis \cite{chu2023qwen}.

We adopt Qwen3-0.6B, a lightweight 600M-parameter causal language model with 28 Transformer layers, as its parameter scale matches the upstream MERT-v1-330M encoder and its compactness balances performance and computational cost.

We feed features from the HDM module into Qwen3-0.6B, which adopts a non-autoregressive generation approach. All pre-trained parameters of the model are frozen, leveraging only its long-range temporal dependency sequence modeling capability to model the frame-level, bar-level, and global-level drum features extracted by the HDM module. We introduce a multi-layer weight fusion strategy: outputs from all Transformer layers are weighted and fused to maximize the retention of the modeled coarse-to-fine temporal music features. These features are then output to the multi-task fine-tuning module.

\subsection{Multi-task Fine-tuning and Loss Function}
Our downstream prediction model comprises a shared global convolution layer and three task-specific branch networks, mapping the feature $H_d \in \mathbb{R}^{T \times F}$ output by the Qwen3 decoder to three tasks: Onset, Frame, and Velocity. We utilize the frame branch to assist the onset branch's prediction, and utilize both the frame and onset branches to assist the velocity prediction. 

To accurately model polyphonic drum events where multiple instruments can be struck simultaneously, we formulate the transcription as a \textbf{multi-label binary classification} problem rather than a standard multi-class classification. This means each drum instrument is treated as an independent binary decision channel. Consequently, all three tasks output predictions with the unified shape of $(N_{pitch}, N_{time})$, as shown in Figure \ref{fig1}, where $N_{pitch}=7$ denotes the number of drum instrument classes, which follows the General MIDI standard where pitches index distinct percussion sounds, and $N_{time}$ represents the number of time frames.

Notably, ADT does not require offset detection, as drums' unique Attack-Decay-Sustain (ADS) envelope—rapid decay post-attack and negligible low-energy sustain—leads to fast energy dissipation, rendering offset ill-defined and irrelevant.

To comprehensively capture all aspects of drum transcription, we adopt multi-task joint optimization, with the total loss $L_{\text{total}}$ being a weighted sum of the three sub-task losses:
\begin{equation}
L_{\text{total}} = L_{\text{onset}} + L_{\text{frame}} + L_{\text{velocity}}
\end{equation}
We assign equal weights to all three tasks to ensure a balanced optimization across onset detection, frame activation, and velocity estimation.

The calculation of each loss is as follows:

\subsubsection{Onset and Frame Loss}
Both $L_{\text{onset}}$ and $L_{\text{frame}}$ employ the standard \textbf{Binary Cross-Entropy} (BCE) loss to handle the frame-wise binary classification. We define the general BCE loss function as:
\begin{equation}
L_{\text{BCE}}(P, Y) = - \frac{1}{N} \sum_{i=1}^{N} [ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) ]
\end{equation}
where $N = N_{time} \times N_{pitch}$ represents the total number of prediction points. $y_i$ and $p_i$ denote the elements of the flattened ground truth $Y$ and prediction $P$ respectively. Consequently, the losses are calculated as $L_{\text{onset}} = L_{\text{BCE}}(P_{onset}, Y_{onset})$ and $L_{\text{frame}} = L_{\text{BCE}}(P_{frame}, Y_{frame})$, where $P_{onset}, P_{frame}$ are the predicted probabilities and $Y_{onset}, Y_{frame}$ are the corresponding ground truth labels for onset and frame tasks.

\subsubsection{Velocity Loss}
For Velocity prediction ($L_{\text{velocity}}$), we use a Weighted Mean Squared Error. To focus learning on valid note events, this loss is calculated only on frames where an onset is present. The formula is:
\begin{equation}
L_{\text{velocity}} = \frac{\sum_{i=1}^{N} y_{\text{onset}, i} \cdot (v_{\text{true}, i} - v_{\text{pred}, i})^2}{\sum_{i=1}^{N} y_{\text{onset}, i} + \epsilon}
\end{equation}
where $v_{\text{true}, i}, v_{\text{pred}, i} \in [0, 1]$ are the normalized true and predicted velocities, $y_{\text{onset}, i} \in \{0, 1\}$ acts as a mask to select only onset-positive frames, and $\epsilon$ is a small constant for numerical stability.

\section{Experiments and Analysis}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt} % 进一步缩小列间距
\begin{tabular}{lccccc} 
\toprule
Model & Training data & Velocity & Note & Note w/ Vel & Frame \\ % 移除F1，Note with Vel缩写为Note w/ Vel
\midrule
% 上半部分：基线模型（Frame列保留-）
OaF Drums & E-GMD & $\checkmark$ & 83.40* & 61.70* & 77.87$^\dagger$ \\
DT-Ensemble & TMIDT & $\times$ & 64.98* & - & - \\
ADTOF & ADTOF & $\times$ & 44.95* & - & - \\
N2N & E-GMD & $\checkmark$ & 89.68* & 82.80* & - \\
Hi-Drum & E-GMD & $\checkmark$ & \textbf{91.31} & \textbf{87.87} &\textbf{97.00}  \\
\bottomrule
\end{tabular}
\caption{\textbf{ADT benchmark results on E-GMD.} Best results in bold. *: From original papers; $\dagger$: From evaluating public trained models. Note, Note w/ Vel, Frame, are F1 scores.}
\label{tab:sota_comparison}
\end{table}



\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{picture/真实转录对比图.png}
\caption{Visualization of transcription results. From top to bottom: Ground Truth, Hi-Drum, and OaF Drums transcription results.}
\label{fig:transcription_vis}
\end{figure*}

\subsection{Experimental Setup}

\textbf{Datasets and Preprocessing}
Two public datasets are used for model training and validation: Groove MIDI Dataset (GMD) \cite{groove_dataset} and Extended Groove MIDI Dataset (E-GMD) \cite{egmd-oaf-drum}. 
GMD (13.6 hours) contains the complete set of human-performed rhythmic patterns (MIDI sequences) recorded on a standard Roland TD-11 kit, while E-GMD (444 hours) expands upon it by re-recording these patterns across 43 different drum kits, offering massive timbral diversity. Characterized by real-world human performance, diverse musical styles, and rich acoustic environments, these datasets are currently recognized as the largest, most comprehensive, and challenging benchmarks in the ADT field.
Both datasets retain train/test/validation splits and high-quality velocity annotations not available in other datasets. To evaluate the model's performance on a standardized drum mapping, we employed a 7-Class Mapping strategy, condensing the drum kit into Kick (KD), Snare (SD), Tom (TT), Hi-Hat (HH), Cymbal (CY), Ride (RD), and Bell (BE), as detailed in Table \ref{tab:mapping}.
\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt} 
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ll cccccc}
\toprule
\multicolumn{8}{l}{\textbf{A. Ablation Study of Components}} \\
\midrule
 & & \multicolumn{3}{c}{\textbf{\tiny GMD}} & \multicolumn{3}{c}{\textbf{\tiny E-GMD}} \\
\multicolumn{2}{l}{Model} & Note & Note w/ Vel & Frame & Note & Note w/ Vel & Frame \\
\midrule
\multicolumn{2}{l}{Hi-Drum w/o HDM \& Multi-task Fine-tuning} & 91.57 & 88.76 & 95.11 & 89.09 & 84.85 & 96.31 \\
\multicolumn{2}{l}{Hi-Drum w/o Multi-task Fine-tuning} & 93.05 & 88.50 & 97.41 & 88.03 & 83.82 & 96.27 \\
\multicolumn{2}{l}{Hi-Drum w/o HDM} & 92.51 & 89.29 & 97.62 & 90.32 & 86.84 & 96.81 \\
\multicolumn{2}{l}{Hi-Drum} & \textbf{94.53} & \textbf{91.41} & \textbf{98.15} & \textbf{91.31} & \textbf{87.87} & \textbf{97.00} \\
\midrule
\multicolumn{8}{l}{\textbf{B. Analysis of Encoder-Decoder Combinations}} \\
\midrule
Encoder & Decoder & Note & Note w/ Vel & Frame & Note & Note w/ Vel & Frame \\
\midrule
\multirow{4}{*}{MERT-330m\cite{mert}} & Hunyuan-0.5B\cite{hunyuan} & 94.11 & 92.19 & 96.06 & - & - & - \\
 & MiniCPM4-0.5B\cite{minicpm4} & 94.20 & 91.80 & 96.69 & - & - & - \\
 & Qwen2.5-0.5B\cite{qwen2.5-coder} & 93.67 & 91.79 & 96.36 & - & - & - \\
 & Qwen3-0.6B\cite{qwen3} & \textbf{94.53} & 91.41 & \textbf{98.15} & - & - & - \\
\midrule
\multirow{4}{*}{MERT-95m\cite{mert}} & Hunyuan-0.5B\cite{hunyuan} & 92.40 & 88.59 & 95.73 & 87.82& 61.41 & 81.83 \\
 & MiniCPM4-0.5B\cite{minicpm4} & 92.07 & 87.29 & 94.35 & 88.62 & 63.08 & 81.37 \\
 & Qwen2.5-0.5B\cite{qwen2.5-coder} & 92.68 & 89.58 & 95.79 & 87.73 & 57.21 & 81.81 \\
 & Qwen3-0.6B\cite{qwen3} & 92.65 & 89.38 & 95.72 & 87.79 & 56.72 & 81.94 \\
\midrule
\multirow{4}{*}{MuQ\cite{muq}} & Hunyuan-0.5B\cite{hunyuan} & 94.46 & 92.31 & 96.62 & 91.89 & 67.37 & 85.14 \\
 & MiniCPM4-0.5B\cite{minicpm4} & 93.80 & 91.71 & 96.18 & 91.68 & 66.45 & 84.84 \\
 & Qwen2.5-0.5B\cite{qwen2.5-coder} & 94.21 & \textbf{92.72} & 96.62 & \textbf{92.21} & 66.84 & \textbf{85.32} \\
 & Qwen3-0.6B\cite{qwen3} & 94.29 & 91.21 & 96.60 & 92.04 & 68.06 & 85.26 \\
\midrule
\multirow{4}{*}{MusicFM\cite{musicfm}} & Hunyuan-0.5B\cite{hunyuan} & 92.30 & 88.89 & 95.63 & 89.58 & \textbf{69.13} & 82.94 \\
 & MiniCPM4-0.5B\cite{minicpm4} & 92.61 & 88.47 & 95.06 & 87.22 & 66.66 & 8089 \\
 & Qwen2.5-0.5B\cite{qwen2.5-coder} & 92.52 & 89.35 & 94.94 & 89.02 & 63.06 & 82.37 \\
 & Qwen3-0.6B\cite{qwen3} & 91.92 & 89.33 & 94.68 & 88.16 & 68.56 & 81.54 \\
\bottomrule
\end{tabular*}
\caption{\textbf{Component ablation (A) and encoder-decoder combinations (B) on GMD and E-GMD.} Best results are in bold.}
\label{tab:model_selection}
\end{table*}
% =========================================================================================
% Mapping Strategy Table
% =========================================================================================
\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{ccccccc}
\toprule
\textbf{KD} & \textbf{SD} & \textbf{TT} & \textbf{HH} & \textbf{CY} & \textbf{RD} & \textbf{BE} \\
\midrule
Kick drum & Snare drum & Tom 1 & Open Hi-Hat & Crash 1 Bow & Ride Bow & Ride Bell \\
 & Snare rim & Tom 1 Rim & Open HH Bow & Crash 1 Edge & Ride Edge & Cow Bell \\
 & Cross-stick & Tom 2 & Closed HH Bow & Crash 2 Bow & & \\
 & Clap & Tom 2 Rim & Closed HH Edge & Crash 2 Edge & & \\
 & & Tom 3 & Pedal Hi-Hat & & & \\
 & & Tom 3 Rim & Tambourine & & & \\
\bottomrule
\end{tabular}
\caption{\textbf{The Mapping Strategy from Groove dataset hits to 7-class target categories.} E-GMD introduces three additional components compared to GMD: Clap, Tambourine, and Cow Bell \protect\cite{groove_dataset}\protect\cite{egmd-oaf-drum}.}
\label{tab:mapping}
\end{table*}

All datasets underwent standardized preprocessing: audio was resampled to 24 kHz. The hop length is set to 320 samples, corresponding to a frame rate of 75 Hz (approximately 13.33 ms per frame).

\textbf{Training Details}
The model was trained for 90,000 iterations using the Adam optimizer (batch size = 8), taking approximately 48 hours.


\textbf{Evaluation Metrics} Following standard protocols, we employ the \texttt{mir\_eval} library \cite{mir_eval} to compute F1 scores. We report Note F1, where a correct prediction requires matching pitch and an onset within $\pm50$ms. For Note F1 (vel), the velocity must additionally be within $\pm10$ of the ground truth. We also provide Frame F1 to measure frame-wise activation accuracy.


\textbf{Baselines} To verify the advancement of our proposed method, we selected the SOTA drum transcription model Noise-to-Notes (N2N) \cite{n2n} as the main comparison baseline. This method, proposed by Yeung et al., redefines ADT as a conditional generation task using diffusion models. In addition, we also introduced the classic model OaF Drums \cite{egmd} for supplementary comparison. This method, proposed by Callender et al., is adapted from the Onsets and Frames architecture and emphasizes the importance of velocity prediction for perceptual quality.





\subsection{Comparative Results}


\label{sec:sota_comparison}

Table \ref{tab:sota_comparison} presents the comparative results on the E-GMD benchmark. We compare our proposed Hi-Drum with several drum transcription models, including N2N \cite{n2n} (current SOTA), OaF Drums \cite{egmd-oaf-drum}, ADTOF \cite{zehren2021adtof}, and DT-Ensemble \cite{vogl2018towards}. As the largest and most timbrally diverse ADT dataset, E-GMD serves as a robust benchmark. As indicated by the results, our proposed Hi-Drum achieves new SOTA levels across all core metrics.

In terms of note-level transcription, which is the most critical metric for ADT, we observed that Hi-Drum achieves 91.31\% Note F1 ($>$ 89.68\% of N2N) and 87.87\% Note F1 (vel) ($>$ 82.80\% of N2N). The improvement in velocity estimation (5.07\%) is significantly more pronounced than in note detection (1.63\%). This validates the effectiveness of our proposed framework, especially considering that velocity determination depends on note recognition—the system first detects the presence of a note and then predicts its velocity. Compared to N2N which generates notes from noise, our method benefits from the HDM's hierarchical feature extraction. By incorporating prior knowledge of local variations and global similarities in drum performances into the modeling process, it effectively captures coarse-to-fine features, thereby achieving superior performance in both note and velocity metrics compared to the SOTA model.

Regarding frame-level accuracy, we observed a dramatic improvement in Frame F1 (97.00\% $>$ 77.87\%) compared to the OaF baseline. Note that the N2N model generates notes from noise via a diffusion process, and its fundamental unit is the note, so it does not provide frame-level metrics for comparison. Since frames form the basis of composing notes, the substantial improvement in frame-level metrics directly contributes to the enhanced note-level performance, further validating the effectiveness of the HDM's hierarchical design in fusing frame-level, bar-level, and global-level feature information.





\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{picture/7class.png}
\caption{Comparison of Note F1 scores for specific drum components between Hi-Drum and OaF Drums under the 7-class mapping.}
\label{fig:class_hist}
\end{figure}

\subsection{Ablation Study}
\label{sec:ablation_study}

We conducted ablation studies on the GMD and E-GMD datasets (Table \ref{tab:model_selection}) to validate our architectural decisions. Part A isolates the impact of the HDM and Multi-task Fine-tuning modules, while Part B compares different foundation model combinations (all incorporating HDM and fine-tuning).

\textbf{Effectiveness of Components}
We validated the contribution of our key components by comparing the full Hi-Drum model with variants lacking the HDM or the Multi-task Fine-tuning module. As shown in Table \ref{tab:model_selection} (A), the HDM module plays a critical role in our framework. On the GMD dataset, removing the HDM results in a significant performance drop, with the Note F1 score decreasing from 94.53\% to 92.51\%. A similar trend is observed on the E-GMD dataset, where the score drops from 91.31\% to 90.32\%. This demonstrates that our hierarchical design effectively captures the essential rhythmic features of drum performances. The Multi-task Fine-tuning module also contributes to the performance, and its removal leads to a drop to 93.05\% on GMD and 88.03\% on E-GMD. The full Hi-Drum architecture achieves the best results across all metrics on both datasets, proving the effectiveness of our design.

\textbf{Analysis of Encoder-Decoder Combinations}
To verify the generalization and robustness of our framework, we selected several representative general music representation models (MERT-95m, MuQ, MusicFM) and LLMs with comparable parameter scales (Hunyuan-0.5B, MiniCPM4-0.5B, Qwen2.5-0.5B) for comparative analysis. As detailed in Table \ref{tab:model_selection} (B), thanks to the effective design of the HDM and Multi-task Fine-tuning modules, our framework achieves excellent performance across all combinations. On the GMD dataset, Note F1 scores consistently exceed 91\%, while on the E-GMD dataset, they remain robust, generally ranging between 87\% and 92\%. It is worth noting that the performance on E-GMD is slightly lower than on GMD. This performance gap is primarily attributed to the significantly larger scale of E-GMD and its inclusion of 43 different drum kits with massive timbral diversity, which poses greater challenges for generalization. Despite this, our proposed architecture effectively bridges the gap between acoustic and symbolic representations, ensuring high-quality transcription regardless of the specific foundation model selection. Among these, the combination of MERT-330m and Qwen3-0.6B yields the comprehensive optimal performance.



\subsection{Analysis on 7-Class Mapping}
Experimental results demonstrate that our model achieves high performance under the 7-Class Mapping strategy (Note F1 $>$ 91\%, Frame F1 $>$ 97\%). Moreover, our model outperforms the OaF Drums baseline across all instrument components (Fig. \ref{fig:class_hist}), with particularly significant improvements in challenging categories like Bell and Cymbal, further confirming the robustness and superiority of our proposed Hi-Drum model and HDM module.






\subsection{Qualitative Analysis}
Fig. \ref{fig:transcription_vis} visualizes the transcription of a complex drum segment. As shown, Hi-Drum successfully captures the global similarities and local variations in drum performances. This capability enables precise drum hit recognition even during complex fills and prevents misestimation of repetitive, similar drum patterns. In contrast, OaF Drums fails to maintain such accuracy, often struggling with these structural nuances.


\section{Conclusion}
This paper presents Hi-Drum, a novel ADT framework leveraging large-scale pre-trained models. By integrating MERT and Qwen3 with a specifically designed Hierarchical Drum Module (HDM) and a multi-task fine-tuning strategy, we achieve effective alignment of acoustic and symbolic features. Experiments on GMD and E-GMD datasets demonstrate that Hi-Drum establishes new state-of-the-art results, surpassing the N2N baseline by 1.63\% in Note F1. Ablation studies confirm the necessity of the HDM and the chosen pre-trained components. This work demonstrates the effectiveness of combining music understanding foundation models and LLMs, significantly advancing the field of drum transcription.

In future work, we plan to investigate the interpretability of the universal music representations extracted by MERT, analyzing the importance of features from different layers—ranging from low-level acoustic to high-level musical semantics—to the model's performance. We also aim to explore the robustness of our approach in noisy real-world recording scenarios. Finally, we intend to leverage our transcription model to construct larger-scale datasets, thereby providing high-quality data to advance the field of automatic drum transcription.

\nocite{gardner2021mt3, stardrums, alonso2025omar, kong2025emergent, ye2025codec, Toyama_2023, Wang_2025, Thapa_2024_12, music_fm_booster, liu2024audioldm, nmf_drum, vogl2018towards, yuan2023marble}

\bibliographystyle{named}
\bibliography{ijcai26}

\end{document}
